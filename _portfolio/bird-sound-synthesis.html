---
title: "ChirpNet - A Neural Bird Sound Synthesis Model"
excerpt: "Automated species recognition through Artificial Intelligence (AI) can provide
ecologists and wildlife conservationists the means to detect and monitor
vocalizing avian species of interest. However, AI systems rely on a multitude
of data to learn the inductive biases and to generalize to the vocalizations of
bird species. This is problematic when classifying rare bird species which
have limited presence in naturally occurring audio data. In this project, we
try to address this problem by generating synthetic avian audio samples as
a type of augmentation that can be used in future works to improve the bird
sound classification accuracy."
collection: portfolio
---

<html>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<h1>Project Members</h1>
<b>Keshav Bhandari (keshavbhandari@gmail.com)</b>
<br>
<b>Tonmoay Deb (tonmoay.deb@u.northwestern.edu)</b>
<br>
<br>
<h1>Course</h1>
<b>Northwestern University</b>
<br>
<b>Deep Generative Models taught by Professor Bryan Pardo</b>
<br>

<h2>What is this project about?</h2>
Automated species recognition through Artificial Intelligence (AI) can provide
ecologists and wildlife conservationists the means to detect and monitor
vocalizing avian species of interest. However, AI systems rely on a multitude
of data to learn the inductive biases and to generalize to the vocalizations of
bird species. This is problematic when classifying rare bird species which
have limited presence in naturally occurring audio data. <b>In this project, we
try to address this problem by generating synthetic avian audio samples as
a type of augmentation that can be used in future works to improve the bird
  sound classification accuracy</b>.

<h2>Motivation and why this project is interesting</h2>
This problem is interesting because of two reasons:
<ul>
  <li>Audio synthesis models have traditionally focused on synthesizing
human speech or on music generation. As such
there are limited models that have been put to the task of
generating environmental sounds or avian sounds. It would
be quite fascinating to see how these models perform at a
high entropy task such as this one.</li>
  <li>Generating sounds that mimic avian calls is inherently difficult.
For example, there are often background environment
related noises coming from insects, wind/rain, humans, vehicles, etc.
Second, bird calls are quite varied and even the
same bird species could make a different sound, depending
on the motive and context. Third, the range of avian calls
across species is also quite large as opposed to human speech.
Finally, bird call datasets often consist of weak labels, where
annotators do not know the precise location of avian calls in
the audio files against silence as compared to text transcriptions in speech datasets.
Thus, there exists ample of challenges and opportunities to address
and take research on synthesizing avian calls to the next level.</li>
</ul>
Thus, there exists ample of challenges and opportunities to address
and take research on synthesizing avian calls to the next level.
<br>

<h3>Intellectual Interest And Practical Utilities</h3>
Every year, there are many species of birds that are put on the
endangered or extinct list. Automated bio-acoustics through the use
of AI provides a more efficient way for humans to measure conservation
efforts to protect wildlife. Thus, by improving the means
to identify bird species as an overarching goal, we are working
towards enhancing environmental conservation and protection of
biodiversity. This project is highly cross-functional and its outcome
could potentially open research avenues to other wildlife species of
interest.
<br>

<h2>Dataset Description</h2>
We use a subset of the xccoverbl dataset in our experiments. This dataset is available on
<a href="https://www.kaggle.com/rtatman/british-birdsong-dataset">Kaggle</a>.
It contains vocalizations of 88 bird species commonly observed in the UK,
gathered from the large Xeno Canto collection. There are 263 audio files in the dataset
with a sampling rate is 44.1kHz. Of these 263 audio files, we manually selected 153 files
containing birds whose vocalizations were distinct and whose audio clips do not suffer
from long periods of silence gaps in between the calls.
<br>

<h2>Model Choice</h2>

<h2>State space models</h2>


<h2>SaShiMi Model Architecture + Our Enhancements</h2>
<img src="/files/audio/bird_sound_synthesis/SaShiMi_Architecture.png" alt="SaShiMi Architecture" style="width:967px;height:400px;">
<i>Karan Goel, Albert Gu, Chris Donahue, Christopher Re Proceedings of the 39th International Conference on Machine Learning, PMLR 162:7616-7633, 2022.</i>
<br><br>
As seen in the image above, the SaShiMi architecture consists of
multiple tiers, with each tier composed of a stack of residual S4 blocks. The top tier processes the raw audio
waveform at its original sampling rate, while lower tiers process downsampled versions of the input signal.
The output of lower tiers is upsampled and combined with the input to the tier above it in order to provide a
stronger conditioning signal. Our enhancements to this architecture is adding a species class conditioning block to the
64-dimensional hidden representations of each timestep of the input signal right before it gets fed into the downsampled block.
Here, in the conditioning block, we have an embedding matrix for the species code followed which is added to the hidden
representation of the audio input and then passed through feed-forward layers with a SiLU activation function.
<br><br>
Adding in this species conditioning improved our validation loss slightly. However, there may be better ways to
implement the class conditioning at strategic locations throughout the network to ensure that it actively maximizes
the influence on the generated timestep.

<h2>Experiments</h2>
We trained the model for 150 epochs on 2,155 5 second audio clips at a 16 kHz sampling rate.
<br>

<h3>Generated Audio Samples</h3>
<table>
  <thead>
    <tr>
      <th align="middle">Unconditional Generation (no prompt)</th>
      <th align="middle">Conditional Generation (1 sec prompt)</th>
    </tr>
  </thead>

   <tbody>
    <tr>
      <td><audio controls style="width: 250px; height: 50px"><source src="/files/audio/bird_sound_synthesis/unconditional_qautoaudio_sashimi_len_5.00s_gen_1.wav" type="audio/wav"></audio></td>
      <td><audio controls style="width: 250px; height: 50px"><source src="/files/audio/bird_sound_synthesis/conditional_qautoaudio_sashimi_len_5.00s_gen_1.wav" type="audio/wav"></audio></td>
    </tr>

    <tr>
      <td><audio controls style="width: 250px; height: 50px"><source src="/files/audio/bird_sound_synthesis/unconditional_qautoaudio_sashimi_len_5.00s_gen_2.wav" type="audio/wav"></audio></td>
      <td><audio controls style="width: 250px; height: 50px"><source src="/files/audio/bird_sound_synthesis/conditional_qautoaudio_sashimi_len_5.00s_gen_2.wav" type="audio/wav"></audio></td>
    </tr>

    <tr>
      <td><audio controls style="width: 250px; height: 50px"><source src="/files/audio/bird_sound_synthesis/unconditional_qautoaudio_sashimi_len_5.00s_gen_3.wav" type="audio/wav"></audio></td>
      <td><audio controls style="width: 250px; height: 50px"><source src="/files/audio/bird_sound_synthesis/conditional_qautoaudio_sashimi_len_5.00s_gen_3.wav" type="audio/wav"></audio></td>
    </tr>

  </tbody>
</table>



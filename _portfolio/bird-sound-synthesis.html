---
title: "ChirpNet - A Neural Bird Sound Synthesis Model"
excerpt: "Automated species recognition through Artificial Intelligence (AI) can provide
ecologists and wildlife conservationists the means to detect and monitor
vocalizing avian species of interest. However, AI systems rely on a multitude
of data to learn the inductive biases and to generalize to the vocalizations of
bird species. This is problematic when classifying rare bird species which
have limited presence in naturally occurring audio data. In this project, we
try to address this problem by generating synthetic avian audio samples as
a type of augmentation that can be used in future works to improve the bird
sound classification accuracy."
collection: portfolio
---

<html>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<h1>Project Members</h1>
<b>Keshav Bhandari (keshavbhandari@gmail.com)</b>
<br>
<b>Tonmoay Deb (tonmoay.deb@u.northwestern.edu)</b>
<br>
<br>
<h1>Course</h1>
<b>Northwestern University</b>
<br>
<b>Deep Generative Models taught by Professor Bryan Pardo</b>
<br>

<h2>What is this project about?</h2>
Automated species recognition through Artificial Intelligence (AI) can provide
ecologists and wildlife conservationists the means to detect and monitor
vocalizing avian species of interest. However, AI systems rely on a multitude
of data to learn the inductive biases and to generalize to the vocalizations of
bird species. This is problematic when classifying rare bird species which
have limited presence in naturally occurring audio data. <b>In this project, we
try to address this problem by generating synthetic avian audio samples as
a type of augmentation that can be used in future works to improve the bird
  sound classification accuracy</b>.

<h2>Motivation and why this project is interesting</h2>
This problem is interesting because of two reasons:
<ul>
  <li>Audio synthesis models have traditionally focused on synthesizing
human speech or on music generation. As such
there are limited models that have been put to the task of
generating environmental sounds or avian sounds. It would
be quite fascinating to see how these models perform at a
high entropy task such as this one.</li>
  <li>Generating sounds that mimic avian calls is inherently difficult.
For example, there are often background environment
related noises coming from insects, wind/rain, humans, vehicles, etc.
Second, bird calls are quite varied and even the
same bird species could make a different sound, depending
on the motive and context. Third, the range of avian calls
across species is also quite large as opposed to human speech.
Finally, bird call datasets often consist of weak labels, where
annotators do not know the precise location of avian calls in
the audio files against silence as compared to text transcriptions in speech datasets.
Thus, there exists ample of challenges and opportunities to address
and take research on synthesizing avian calls to the next level.</li>
</ul>
Thus, there exists ample of challenges and opportunities to address
and take research on synthesizing avian calls to the next level.
<br>

<h3>Intellectual Interest And Practical Utilities</h3>
Every year, there are many species of birds that are put on the
endangered or extinct list. Automated bio-acoustics through the use
of AI provides a more efficient way for humans to measure conservation
efforts to protect wildlife. Thus, by improving the means
to identify bird species as an overarching goal, we are working
towards enhancing environmental conservation and protection of
biodiversity. This project is highly cross-functional and its outcome
could potentially open research avenues to other wildlife species of
interest.
<br>

<h2>Dataset Description</h2>
We use a subset of the xccoverbl dataset in our experiments.
It contains vocalizations of 88 bird species commonly observed in the UK,
gathered from the large Xeno Canto collection. There are 263 audio files in the dataset
with a sampling rate is 44.1kHz. Of these 263 audio files, we manually selected 153 files
containing birds whose vocalizations were distinct and whose audio clips do not suffer
from long periods of silence gaps in between the calls.
<br>

Experiments
-State space models
-SaShiMi

Model Results
-Audio Samples

